{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef702e",
   "metadata": {},
   "source": [
    "#### REINFORCE: Policy Gradients Algorithm implementation on CARTPOLE problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the environment \n",
    "env = gym.make('CartPole-v1')\n",
    "for _ in range(10):\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8fe47",
   "metadata": {},
   "source": [
    "### a) Defining the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a78ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        '''In init we define the layers'''\n",
    "        self.hidden1 = nn.Linear(in_features=4, out_features=128)\n",
    "        self.output = nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.hidden1(x))      # RELU non linearity\n",
    "        x = self.output(x)               # final output layer\n",
    "        x = F.softmax(x,dim=1)           # bcz we need prob dist over action space--> dim = 1 bcz (input is in shape=[1,4])\n",
    "                                         # if input was in shape[4], then dim=0 needs to be mention                              \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "_test_output = Policy()(torch.tensor([[1.0, 2, 3, 4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "np.testing.assert_almost_equal(_test_output.detach().numpy().sum(), 1, err_msg=\"Output is not a probability distribution.\")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f8a13",
   "metadata": {},
   "source": [
    "### b) Action Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901684eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(probs):\n",
    "    \"\"\"Sample one action from the action distribution of this state.\n",
    "    \n",
    "    Args:\n",
    "        probs: action probabilities\n",
    "\n",
    "    Returns:\n",
    "        action: The sampled action\n",
    "        log_prob: Logarithm of the probability for sampling that action\n",
    "    \"\"\"\n",
    "    # TODO Implement action sampling\n",
    "    ''' \n",
    "    used the code given under pyTorch documentation\n",
    "        # Taking the list of probs (output of softmax and gives back the distribution of action space i.e action pdf)\n",
    "        # Sample the distribution to get action (idx = dist.sample()) (those action will get selected who has higher probability)-->(return index value of the action)\n",
    "        # Evaluate the selected action using dist.log_prob(idx) == np.log(probs[idx])\n",
    "    '''\n",
    "    dist = torch.distributions.Categorical(probs)           \n",
    "    action = dist.sample()                         # return the idx where prob is max\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b726da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "_test_action, _test_log_prob = sample_action(torch.tensor([1, 2, 3, 4]))\n",
    "assert _test_action in [0, 1, 2, 3], f\"Invalid action {_test_action}\"\n",
    "np.testing.assert_approx_equal(_test_log_prob, np.log((_test_action + 1) / 10))\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f76ef",
   "metadata": {},
   "source": [
    "### c) Return Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f835e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_return(rewards, gamma=0.99):\n",
    "\n",
    "    \"\"\"Estimate return based of observed rewards\n",
    "    \n",
    "    Args:\n",
    "        rewards: Series of observed rewards\n",
    "        gamma: discount factor\n",
    "      \n",
    "    G(t) = rt + G(t+1)\n",
    "    \"\"\"\n",
    "\n",
    "    discount_rewards = np.array(rewards)  \n",
    "    for step in range(len(rewards)-2,-1,-1):\n",
    "        discount_rewards[step] = discount_rewards[step] + gamma*discount_rewards[step+1]\n",
    "    \n",
    "    ## apply standardization\n",
    "    '''\n",
    "    discount_rewards[0] = (discount_rewards[0] - mean) / std\n",
    "    '''\n",
    "    mean_value = discount_rewards.mean()\n",
    "    std_value = discount_rewards.std()\n",
    "    discount_rewards = (discount_rewards-mean_value)/std_value\n",
    "\n",
    "    return torch.from_numpy(discount_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153595ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "np.testing.assert_array_almost_equal(\n",
    "    estimate_return(np.ones(10), gamma=0.99),\n",
    "    [1.54572815, 1.21139962, 0.87369404, 0.53257729, 0.18801491, -0.16002789, -0.51158628, -0.86669576, -1.22539221, -1.58771185]\n",
    ")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73efaa1",
   "metadata": {},
   "source": [
    "### d) Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "policy = Policy()\n",
    "\n",
    "# Hyperparams\n",
    "episodes = 1000\n",
    "gamma = 0.98\n",
    "learn_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learn_rate)\n",
    "\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        # Run one episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            # ********************\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)           # will return the state in torch.tensor size [1,4]\n",
    "            probs = policy.forward(state)                                  # gets prob of action space\n",
    "            action, log_prob = sample_action(probs)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Bookkeeping\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # ********************\n",
    "        # Compute loss\n",
    "        G = estimate_return(rewards)                 # getting the discounted reward (dtype= torch.tensor)\n",
    "        policy_loss = []\n",
    "        for i in range(len(log_probs)):\n",
    "            policy_loss.append(-log_probs[i]*G[i])\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        # ********************\n",
    "\n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/a2d.pt\", \"wb\") as f:\n",
    "    torch.save(policy, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.seed(seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "\n",
    "\n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy = True\n",
    "\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "    if greedy:\n",
    "        action = np.argmax(probs.detach().numpy())  # Chose optimal action\n",
    "    else:\n",
    "        action = sample_action(probs)[0]  # Sample from action distribution\n",
    "    state, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
